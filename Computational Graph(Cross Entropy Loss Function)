import numpy as np

# Define Sum()

class Sum:
  def forward(self, x):
    self.x = x
    
    
    return np.sum(self.x)
    
  def backward(self):
    
    dx = []
    for i in range(len(self.x)):
        dx.append(1)
    
    return np.array(dx)




## Cross Entropy Loss Function for Multi-class Classification (class_number = 3)
## equation = -‚àë(y_c) * log(ùë¶ÃÇ_c) (c=1, 2, 3)





class Cross_entropy:

    def forward(self, y_ce, y_predict_ce):
    
        self.log = Log()
        self.multiply1 = Multiply()
        self.sum = Sum()
        self.multiply2 = Multiply()
        self.y_predict_ce = np.array(y_predict_ce)
        self.y_ce = np.array(y_ce)
        
        forward1 = self.log.forward(self.y_predict_ce)
        forward2 = self.multiply1.forward(forward1, self.y_ce)
        forward3 = self.sum.forward(forward2)
        forward4 = self.multiply2.forward(forward3, -1)
        
        self.forward4 = forward4
        
        return self.forward4
        
    def backward(self):
    
        backward1 = self.multiply2.backward()[0]
        backward2 = self.sum.backward() * backward1
        backward3 = self.multiply1.backward()[0] * backward2
        backward4 = self.log.backward() * backward3
        
        return backward4




## example

cross_entropy = Cross_entropy()

forward2 = cross_entropy.forward(y_predict_ce = [2, 3, 4], y_ce = [0, 1, 2])
forward2
# Ï∂úÎ†• = -3.87

cross_entropy.backward()
# Ï∂úÎ†• = [0, -1/3, -0.5]

